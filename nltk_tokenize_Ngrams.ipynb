{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# importing required libraries and datasets\n",
        "import pandas as pd\n",
        "import math\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WB4_W6a_HPdC",
        "outputId": "478118d5-37e2-4640-dfb9-0c0b59f566d9"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.lm import MLE\n",
        "\n",
        "file_path = '/content/drive/MyDrive/raw_reddit_data_filtered.csv'\n",
        "\n",
        "# Reading the file using pandas\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Print the first 10 comments\n",
        "print(df.head(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5ks_rO0HdwS",
        "outputId": "5af8e2b7-9b9c-4bd2-ab18-3ed490be8ab9"
      },
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Unnamed: 0  Post_ID Comment_ID  \\\n",
            "0           1  108l3ho    j3vlnat   \n",
            "1           2  10j6oqj    j5j34zb   \n",
            "2           3  10j6oqj    j5jbs3f   \n",
            "3           4  10j6oqj    j5jfczd   \n",
            "4           5  10o9tvi    j6h1ko8   \n",
            "5           6  10o9tvi    j88hrgq   \n",
            "6           7  10o9tvi    jh0junq   \n",
            "7           8  10o9tvi    j94k8o3   \n",
            "8           9  10o9tvi    j94kbno   \n",
            "9          10  10yjhn9    j7yqwxx   \n",
            "\n",
            "                                             Comment Majority_Sentiment  \n",
            "0  Some of them yes but this one i got it from my...            Neutral  \n",
            "1        I keep mine in my phone cover for good luck           Positive  \n",
            "2  Hang on to it! These are rare to come by as th...            Neutral  \n",
            "3  Yeah?! Cool, I got this from my classmate in 2...           Positive  \n",
            "4  Check out the Kaja Throm! Beautiful marketplac...           Positive  \n",
            "5  Out of curiosity what field of work brings you...           Positive  \n",
            "6  Definitely Dechenphu Lhakhang/Goenkhang! Seren...           Positive  \n",
            "7                     thank you! been to Kaja Throm.           Positive  \n",
            "8  I am in sales...technical sales. So trying to ...            Neutral  \n",
            "9  \"shall not get facilities that other citizens ...           Negative  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# We will do preprocessing by removing links, special characters(all except alphanumeric)\n",
        "regex_pattern_for_HTTPlinks = r'http[s]?:\\S+'\n",
        "regex_pattern_for_WWWlinks = r'www\\S+'\n",
        "regex_for_removing_hashtags = r'#\\S+'\n",
        "all_characters_expect_alphanumeric = r'[^A-Za-z0-9\\s]'\n",
        "\n",
        "comments = []\n",
        "processed_comments = []\n",
        "for _, row in df.iterrows():\n",
        "  comments += sent_tokenize(row[\"Comment\"])\n",
        "\n",
        "for comment in comments:\n",
        "  comment = re.sub(regex_pattern_for_HTTPlinks, '', comment)\n",
        "  comment = re.sub(regex_pattern_for_WWWlinks, '', comment)\n",
        "  comment = re.sub(regex_for_removing_hashtags, '', comment)\n",
        "  comment = re.sub(all_characters_expect_alphanumeric, '', comment)\n",
        "  comment = comment.lower()\n",
        "  comment = comment.strip().split(\" \")\n",
        "  if (len(comment) > 0):\n",
        "    processed_comments.append(comment)"
      ],
      "metadata": {
        "id": "loY5KoAE5f6H"
      },
      "execution_count": 153,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set a random seed for reproducibility\n",
        "np.random.seed(55)\n",
        "processed_comments = np.array(processed_comments)\n",
        "# Shuffle the sentences randomly\n",
        "np.random.shuffle(processed_comments)\n",
        "\n",
        "# Calculate the index for the split (80% for training)\n",
        "split_index = int(0.8 * len(processed_comments))\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "processed_corpus = list(processed_comments[:split_index])\n",
        "processed_test = list(processed_comments[split_index:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlmEnNGVNwsA",
        "outputId": "01f23939-52fb-4cf2-9b21-aacc34dd5883"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-154-c836f18293e6>:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  processed_comments = np.array(processed_comments)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "processed_corpus[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOH8G_cFH6k_",
        "outputId": "8a6532ef-6d29-4fda-937d-37efd8b98e91"
      },
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['please',\n",
              "  'great',\n",
              "  'savior',\n",
              "  'show',\n",
              "  'us',\n",
              "  'ignorant',\n",
              "  'monkeys',\n",
              "  'just',\n",
              "  'how',\n",
              "  'one',\n",
              "  'can',\n",
              "  'resist',\n",
              "  'the',\n",
              "  'depredations',\n",
              "  'of',\n",
              "  'a',\n",
              "  'military',\n",
              "  'regime',\n",
              "  'which',\n",
              "  'the',\n",
              "  'world',\n",
              "  'has',\n",
              "  'ignored',\n",
              "  'for',\n",
              "  '60',\n",
              "  'years'],\n",
              " ['watching', 'alot', 'of', 'korean', 'reality', 'shows\\n\\nsubtitled'],\n",
              " ['assume',\n",
              "  'i',\n",
              "  'am',\n",
              "  'a',\n",
              "  'deadbrained',\n",
              "  'simpleton',\n",
              "  'but',\n",
              "  'sincere',\n",
              "  'who',\n",
              "  'took',\n",
              "  'the',\n",
              "  'picture'],\n",
              " ['great',\n",
              "  'new',\n",
              "  'purchase',\n",
              "  'orders',\n",
              "  'on',\n",
              "  'the',\n",
              "  'way',\n",
              "  'to',\n",
              "  'military',\n",
              "  'industrial',\n",
              "  'complex'],\n",
              " ['no', 'not', 'by', 'a', 'far', 'fetch'],\n",
              " ['certainly', 'not', 'just', 'india'],\n",
              " ['and', 'now', 'were', 'being', 'sold', 'ads', 'based', 'on', 'faith'],\n",
              " ['folks',\n",
              "  'the',\n",
              "  'equation',\n",
              "  'is',\n",
              "  'simple\\n\\nif',\n",
              "  'you',\n",
              "  'mock',\n",
              "  'someones',\n",
              "  'religious',\n",
              "  'belief',\n",
              "  'or',\n",
              "  'belittle',\n",
              "  'their',\n",
              "  'object',\n",
              "  'of',\n",
              "  'devotion',\n",
              "  'then',\n",
              "  'you',\n",
              "  'are',\n",
              "  'an',\n",
              "  'asshole'],\n",
              " ['18', 'lakh', 'for', '10th', 'grade'],\n",
              " ['cant',\n",
              "  'imagine',\n",
              "  'jokers',\n",
              "  'here',\n",
              "  'are',\n",
              "  'commenting',\n",
              "  'in',\n",
              "  'favour',\n",
              "  'of',\n",
              "  'a',\n",
              "  'traitor']]"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lenght of train and test corpus"
      ],
      "metadata": {
        "id": "EUuuMNuWH7hi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(processed_corpus))\n",
        "print(len(processed_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_6tlzOZH7Mh",
        "outputId": "41a747ca-f477-4396-c544-70e62d57ec26"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "268411\n",
            "67103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fucntion for generating n grams within order n"
      ],
      "metadata": {
        "id": "wYZ9U5aaIler"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from copy import deepcopy\n",
        "def everygram_creator(order,processed_corpus):    # function for creating every  ngrams of length <= order\n",
        "\n",
        "  copy_corpus = deepcopy(processed_corpus)  # creating copy of the dataset so  orginal do\n",
        "\n",
        "  for i in range(len(copy_corpus)):\n",
        "    # padding each sentence token according to order\n",
        "\n",
        "    copy_corpus[i]  =  [\"<s>\"]*(order-1) + copy_corpus[i] + [\"</s>\"]*(order-1)\n",
        "\n",
        "  #  creating n grams\n",
        "  n_grams = []\n",
        "  vocab =set()\n",
        "\n",
        "\n",
        "  for sentence in copy_corpus:\n",
        "    for token in sentence:\n",
        "      vocab.add(token)\n",
        "\n",
        "  for i in copy_corpus :\n",
        "    grams = []\n",
        "    for k in range(0,len(i)-order+1):\n",
        "      window = i[k:k+order]    # creating a window of size == order\n",
        "      for j in range(1,len(window)+1):\n",
        "        grams.append(tuple(window[0:j]))  # creating every gram of lenght<= order\n",
        "    n_grams.append(tuple(grams))\n",
        "\n",
        "  return tuple(n_grams),vocab  # return a nested tuple containing all n-grams\n",
        "\n",
        "# An example that how we will create all n grams starting from order 2 on first sentence of the corpus\n",
        "print(\"Sentence:\", processed_corpus[0:1])\n",
        "print()\n",
        "print(\"All bigrams and unigrams of the sentence-\")\n",
        "print(everygram_creator(2, processed_corpus[:1])[0])"
      ],
      "metadata": {
        "id": "62gjjL0vIlNq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4839c1ce-f248-4810-9e7a-a456c3cabf30"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: [['please', 'great', 'savior', 'show', 'us', 'ignorant', 'monkeys', 'just', 'how', 'one', 'can', 'resist', 'the', 'depredations', 'of', 'a', 'military', 'regime', 'which', 'the', 'world', 'has', 'ignored', 'for', '60', 'years']]\n",
            "\n",
            "All bigrams and unigrams of the sentence-\n",
            "((('<s>',), ('<s>', 'please'), ('please',), ('please', 'great'), ('great',), ('great', 'savior'), ('savior',), ('savior', 'show'), ('show',), ('show', 'us'), ('us',), ('us', 'ignorant'), ('ignorant',), ('ignorant', 'monkeys'), ('monkeys',), ('monkeys', 'just'), ('just',), ('just', 'how'), ('how',), ('how', 'one'), ('one',), ('one', 'can'), ('can',), ('can', 'resist'), ('resist',), ('resist', 'the'), ('the',), ('the', 'depredations'), ('depredations',), ('depredations', 'of'), ('of',), ('of', 'a'), ('a',), ('a', 'military'), ('military',), ('military', 'regime'), ('regime',), ('regime', 'which'), ('which',), ('which', 'the'), ('the',), ('the', 'world'), ('world',), ('world', 'has'), ('has',), ('has', 'ignored'), ('ignored',), ('ignored', 'for'), ('for',), ('for', '60'), ('60',), ('60', 'years'), ('years',), ('years', '</s>')),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a dataset for visualizing the trend of average perplexity scores\n",
        "perplexity_data = {\"train_perp_without_LPsmoothing\": [],\n",
        "                   \"train_perp_with_LPsmoothing\": [],\n",
        "                   \"test_perp_with_LPsmoothing\": []}"
      ],
      "metadata": {
        "id": "lz7j6--eMwX5"
      },
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average perplexity of a Unigram LM without smoothing on the train dataset"
      ],
      "metadata": {
        "id": "_rZi_3nQk2gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnigramModel:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.unigram_freq = {}\n",
        "        self.total_tokens = 0\n",
        "\n",
        "    def fit(self, processed_corpus):\n",
        "        for sentence in processed_corpus:\n",
        "            for token in sentence:\n",
        "              self.total_tokens += 1\n",
        "              if token in self.unigram_freq:\n",
        "                  self.unigram_freq[token] += 1\n",
        "              else:\n",
        "                  self.unigram_freq[token] = 1\n",
        "\n",
        "    def unigram_probability(self, token):\n",
        "        if token in self.unigram_freq:\n",
        "            return np.log2(self.unigram_freq[token]) - np.log2(self.total_tokens)\n",
        "        else:\n",
        "            return np.log2(0)\n",
        "\n",
        "    def perplexity(self, processed_test):\n",
        "        perplexity_values = []\n",
        "        for sentence in processed_test:\n",
        "            if len(sentence) > 0:\n",
        "                sentence_probability = 0.0\n",
        "                for token in sentence:\n",
        "                    token_probability = self.unigram_probability(token)\n",
        "                    sentence_probability += token_probability\n",
        "                perplexity_values.append(2 ** (-sentence_probability/len(sentence)))\n",
        "\n",
        "        if len(perplexity_values) > 0:\n",
        "            return sum(perplexity_values) / len(perplexity_values)\n",
        "\n",
        "\n",
        "model = UnigramModel()\n",
        "model.fit(processed_corpus[:])\n",
        "\n",
        "#Perplexity of unsmoothed unigram on test data\n",
        "perplexity = model.perplexity(processed_test[:])\n",
        "print(\"Average perplexity of a Unigram model on the test dataset:\", perplexity)\n",
        "\n",
        "# Perplexity on train\n",
        "perplexity = model.perplexity(processed_corpus[:])\n",
        "print(\"Average perplexity of a Unigram model on the train dataset:\", perplexity)\n",
        "perplexity_data[\"train_perp_without_LPsmoothing\"].append(perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4q6HEmRfGuk",
        "outputId": "edc7b072-9393-4a4a-9ab2-eaab36f5152d"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-159-e5de3bf0fdb9>:20: RuntimeWarning: divide by zero encountered in log2\n",
            "  return np.log2(0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average perplexity of a Unigram model on the test dataset: inf\n",
            "Average perplexity of a Unigram model on the train dataset: 14255.955668687251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unigram with Laplace smoothing"
      ],
      "metadata": {
        "id": "z9ZgtAstky00"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UnigramSmoothedModel:\n",
        "\n",
        "    def __init__(self):\n",
        "      self.unigram_freq = {}\n",
        "      self.total_tokens = 0\n",
        "      self.vocab = set()\n",
        "\n",
        "\n",
        "    def fit(self, processed_corpus):\n",
        "        for sentence in processed_corpus:\n",
        "            for token in sentence:\n",
        "                self.total_tokens += 1\n",
        "                if token in self.unigram_freq:\n",
        "                    self.unigram_freq[token] += 1\n",
        "                else:\n",
        "                    self.unigram_freq[token] = 1\n",
        "                    self.vocab.add(token)\n",
        "\n",
        "    def unigram_probability(self, token):\n",
        "        if token in self.unigram_freq:\n",
        "            return np.log2(self.unigram_freq[token]) - np.log2(self.total_tokens)\n",
        "        else:\n",
        "            return -np.log2(self.total_tokens + len(self.vocab))\n",
        "\n",
        "    def perplexity(self, processed_test):\n",
        "        perplexity_values = []\n",
        "        for sentence in processed_test:\n",
        "            if len(sentence) > 0:\n",
        "                sentence_probability = 0.0\n",
        "                for token in sentence:\n",
        "                    token_probability = self.unigram_probability(token)\n",
        "                    sentence_probability += token_probability\n",
        "                perplexity_values.append(2 ** (-sentence_probability/len(sentence)))\n",
        "\n",
        "        if len(perplexity_values) > 0:\n",
        "            return sum(perplexity_values) / len(perplexity_values)\n",
        "\n",
        "model = UnigramSmoothedModel()\n",
        "model.fit(processed_corpus)\n",
        "\n",
        "perplexity = model.perplexity(processed_test)\n",
        "print(\"Average perplexity of a Smoothed Unigram model on the test dataset:\", perplexity)\n",
        "perplexity_data[\"test_perp_with_LPsmoothing\"].append(perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4Qpm22kkyeE",
        "outputId": "3f78e83d-ef71-43b6-86c6-e090435fee0a"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average perplexity of a Smoothed Unigram model on the test dataset: 17603.90109005058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "N grams generalized class for bigram, trigram, and quadgram"
      ],
      "metadata": {
        "id": "8sAITi3FIDVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ngram():\n",
        "\n",
        "  def __init__(self, order):\n",
        "    self.everygram_to_freq = None\n",
        "    self.unigram_count = 0\n",
        "    self.vocab = None\n",
        "    self.order = order\n",
        "\n",
        "  def fit(self, processed_corpus):\n",
        "\n",
        "    train, vocab = everygram_creator(self.order, processed_corpus)  # creating n-grams of length <= order\n",
        "    self.vocab = vocab\n",
        "    everygram = []\n",
        "    for i in train:\n",
        "        everygram.extend(list(i))\n",
        "    self.everygram_to_freq = {}\n",
        "\n",
        "    # mapping every n gram to its frequency\n",
        "    for i in everygram:\n",
        "      if len(i) == 1:\n",
        "        self.unigram_count += 1\n",
        "\n",
        "      if i not in self.everygram_to_freq:\n",
        "        self.everygram_to_freq[i] = 1\n",
        "      else:\n",
        "        self.everygram_to_freq[i] += 1\n",
        "\n",
        "  def log_prob(self, sentence):\n",
        "\n",
        "    test,v = everygram_creator(self.order, [sentence])\n",
        "    grams = []\n",
        "    for i in test:\n",
        "      grams.extend(list(i))\n",
        "    ind = []\n",
        "\n",
        "    for i in range(len(grams)):\n",
        "      if len(grams[i]) < self.order:\n",
        "        ind.append(i)  # storing indices of n grams\n",
        "\n",
        "    # finding log probability to prevent overflow or underflow error\n",
        "    log_prob_sen = 0\n",
        "    for i in range(len(grams)):\n",
        "      if i in ind:\n",
        "        continue\n",
        "      gram = list(grams[i])\n",
        "      prob_gram = 0\n",
        "      if tuple(gram) in self.everygram_to_freq:\n",
        "        if len(gram) >= 2:  # calculating n-gram conditional probability\n",
        "          prob_gram = np.log2(self.everygram_to_freq[tuple(gram)]) - np.log2(self.everygram_to_freq[tuple(gram[0:-1])])\n",
        "        elif len(gram) == 1:  # counting unigram probability for a unigram\n",
        "          prob_gram = np.log2(self.everygram_to_freq[tuple(gram)]) - np.log2(self.unigram_count)\n",
        "\n",
        "      else:\n",
        "        prob_gram = np.log2(0)\n",
        "      log_prob_sen += prob_gram\n",
        "    return log_prob_sen\n",
        "\n",
        "\n",
        "  def perplexity(self, processed_test):\n",
        "    perplexity_values = []\n",
        "    for tokens in processed_test:\n",
        "      n = len(tokens)\n",
        "      if n >=1:\n",
        "        score = self.log_prob(tokens)\n",
        "        score = 2**(-score/n)  # taking antilog to find initial perplexity\n",
        "\n",
        "        perplexity_values.append(score)\n",
        "\n",
        "    perplexity = np.mean(perplexity_values) # taking the average of all perplexities\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "ELp1MnXgLq7A"
      },
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average perplexity of a Unigram model on the train dataset(unsmoothed)"
      ],
      "metadata": {
        "id": "SbpJyk8IF7Wq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ngram(1)\n",
        "model.fit(processed_corpus[:])\n",
        "perplexity = model.perplexity(processed_corpus[:])\n",
        "print(\"Average perplexity of a Unigram model on the train dataset(unsmoothed):\", perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tosKH0j7FxYm",
        "outputId": "91dcbfd7-4c8a-4d7a-e409-32bec64e2ec4"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average perplexity of a Unigram model on the train dataset(unsmoothed): 14255.955668686127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average perplexity of the train dataset for bigram LM(unsmoothed)"
      ],
      "metadata": {
        "id": "oIdEbEyyIV6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ngram(2)\n",
        "model.fit(processed_corpus[:])\n",
        "perplexity = model.perplexity(processed_corpus[:])\n",
        "print(\"Average perplexity of a Bigram model on the train dataset(unsmoothed):\", perplexity)\n",
        "perplexity_data[\"train_perp_without_LPsmoothing\"].append(perplexity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zu0_4kWFIVqz",
        "outputId": "a201c202-72d5-41d2-dc69-4405771fe924"
      },
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average perplexity of a Bigram model on the train dataset(unsmoothed): 3965.5393335785398\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average perplexity of a Trigram model on the train dataset(unsmoothed)"
      ],
      "metadata": {
        "id": "2883m2aNIVbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ngram(3)\n",
        "model.fit(processed_corpus)\n",
        "perplexity = model.perplexity(processed_corpus)\n",
        "print(\"Average perplexity of a Trigram model on the train dataset(unsmoothed):\", perplexity)\n",
        "perplexity_data[\"train_perp_without_LPsmoothing\"].append(perplexity)"
      ],
      "metadata": {
        "id": "mR8v6U4KIVMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average perplexity of a Quadgram model on the train dataset(unsmoothed)"
      ],
      "metadata": {
        "id": "PCQpB7mhIU5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ngram(4)\n",
        "model.fit(processed_corpus)\n",
        "perplexity = model.perplexity(processed_corpus)\n",
        "print(\"Average perplexity of a Quadgram model on the train dataset(unsmoothed):\", perplexity)\n",
        "perplexity_data[\"train_perp_without_LPsmoothing\"].append(perplexity)"
      ],
      "metadata": {
        "id": "S3-neDOJIUdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideally, when we try to get perplexity scores of these unsmoothed (Uni, Bi, Tri, Quad) Language Models on the test dataset, the scores should approach infinity because:\n",
        "- These LMs are fitted on the train dataset. Hence, there are many n-grams in the test dataset that do not occur in traina and hence will have probability = 0 according to the LMs.\n",
        "- This will lead to 0 in denominator of the perplexity formula and perplexity will reach infinity.\n",
        "- To avoid ''' Division by Zero error''', we give a very less probability (of order e<sup>-25</sup>) to all those n-grams that have a probability = 0.\n",
        "- Hence, we can see that the perplexity scores are very high(approaching infinity) in the order of e<sup>30</sup> for all unsmoothed LMs just as expected."
      ],
      "metadata": {
        "id": "XjMU_i3NcWox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsmoothed Bigram model on test dataset"
      ],
      "metadata": {
        "id": "KkppYPfylaVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ngram(2)\n",
        "model.fit(processed_corpus)\n",
        "perplexity = model.perplexity(processed_test)\n",
        "print(perplexity)"
      ],
      "metadata": {
        "id": "PEnSW2r5OxBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsmoothed Trigram model on test dataset"
      ],
      "metadata": {
        "id": "epjkQzqwldZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ngram(3)\n",
        "model.fit(processed_corpus)\n",
        "perplexity = model.perplexity(processed_test)\n",
        "print(perplexity)"
      ],
      "metadata": {
        "id": "RRLj4yyTOw-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unsmoothed Quadgram model on test dataset"
      ],
      "metadata": {
        "id": "abIK-YivlfJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ngram(4)\n",
        "model.fit(processed_corpus)\n",
        "perplexity = model.perplexity(processed_test)\n",
        "print(perplexity)"
      ],
      "metadata": {
        "id": "sJSbenlPOw5W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Smoothing of N-grams (bigrams, trigrams, quadgram)"
      ],
      "metadata": {
        "id": "cb8s3w2qC7Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ngram_smoothed():\n",
        "\n",
        "  def __init__(self, order):\n",
        "    self.everygram_to_freq = None\n",
        "    self.unigram_count = 0\n",
        "    self.vocab = None\n",
        "    self.order = order\n",
        "\n",
        "  def fit(self, processed_corpus):\n",
        "\n",
        "    train, vocab = everygram_creator(self.order, processed_corpus)  # creating n-grams of length <= order\n",
        "    self.vocab = vocab\n",
        "    everygram = []\n",
        "    for i in train:\n",
        "        everygram.extend(list(i))\n",
        "    self.everygram_to_freq = {}\n",
        "\n",
        "    # mapping every n gram to its frequency\n",
        "    for i in everygram:\n",
        "      if len(i) == 1:\n",
        "        self.unigram_count += 1\n",
        "\n",
        "      if i not in self.everygram_to_freq:\n",
        "        self.everygram_to_freq[i] = 1\n",
        "      else:\n",
        "        self.everygram_to_freq[i] += 1\n",
        "\n",
        "  def log_prob(self, sentence):\n",
        "\n",
        "    test,v = everygram_creator(self.order, [sentence])\n",
        "    grams = []\n",
        "    # grams = list(test)\n",
        "    for i in test:\n",
        "      grams.extend(list(i))\n",
        "    ind = []\n",
        "\n",
        "    for i in range(len(grams)):\n",
        "      if len(grams[i]) == self.order:\n",
        "        ind.append(i)  # storing indices of n grams\n",
        "\n",
        "    log_prob_sen = 0\n",
        "    for i in ind:\n",
        "      gram = list(grams[i])\n",
        "      prob_gram = 0\n",
        "      if tuple(gram) in self.everygram_to_freq:\n",
        "\n",
        "        if len(gram) >= 2:  # calculating n-gram conditional probability\n",
        "          prob_gram = np.log2(self.everygram_to_freq[tuple(gram)] + 1) - np.log2(self.everygram_to_freq[tuple(gram[0:-1])] + len(self.vocab)-2)\n",
        "\n",
        "        elif len(gram) == 1:  # counting unigram probability for a unigram\n",
        "        # Incrementing numerater by 1 and denominator by V = size of the vocabulary\n",
        "          prob_gram = np.log2(self.everygram_to_freq[tuple(gram)] + 1) - np.log2(self.unigram_count + len(self.vocab))\n",
        "      else:\n",
        "        if (len(gram) >= 2):\n",
        "          if (tuple(gram[0:-1]) in self.everygram_to_freq):\n",
        "            prob_gram = -np.log2(self.everygram_to_freq[tuple(gram[0:-1])] + len(self.vocab)-2)\n",
        "          else:\n",
        "            prob_gram = -np.log2(len(self.vocab)-2)\n",
        "        else:\n",
        "          prob_gram = -np.log2(self.unigram_count + len(self.vocab))\n",
        "      log_prob_sen += prob_gram\n",
        "    return log_prob_sen\n",
        "\n",
        "\n",
        "  def perplexity(self, processed_test):\n",
        "    perplexity_values = []\n",
        "    for tokens in processed_test:\n",
        "      n = len(tokens)\n",
        "      if n >=1:\n",
        "        score = self.log_prob(tokens)\n",
        "        score = 2**(-score/n)  # taking antilog to find initial perplexity\n",
        "\n",
        "        perplexity_values.append(score)\n",
        "\n",
        "    perplexity = np.mean(perplexity_values) # taking the average of all perplexities\n",
        "    return perplexity"
      ],
      "metadata": {
        "id": "NrvvKwLsDADP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With smoothing on bigrams"
      ],
      "metadata": {
        "id": "tWYuKx-XDXo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ngram_smoothed(2)\n",
        "model.fit(processed_corpus)\n",
        "perplexity = model.perplexity(processed_test)\n",
        "print(\"Average perplexity of test dataset on Laplace smoothed bigram fitted on train dataset:\", perplexity)\n",
        "perplexity_data[\"test_perp_with_LPsmoothing\"].append(perplexity)"
      ],
      "metadata": {
        "id": "oUIEoTi_DaIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smotthing on trigrams"
      ],
      "metadata": {
        "id": "URGxh_fGFVt0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ngram_smoothed(3)\n",
        "model.fit(processed_corpus)\n",
        "perplexity = model.perplexity(processed_test)\n",
        "print(\"Average perplexity of test dataset on Laplace smoothed trigram fitted on train dataset:\",perplexity)\n",
        "perplexity_data[\"test_perp_with_LPsmoothing\"].append(perplexity)"
      ],
      "metadata": {
        "id": "BFfd2Mq6Dhe6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Smotthing on Quadgrams"
      ],
      "metadata": {
        "id": "VuYMGquyFbxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = ngram_smoothed(4)\n",
        "model.fit(processed_corpus)\n",
        "perplexity = model.perplexity(processed_test)\n",
        "print(\"Average perplexity of test dataset on Laplace smoothed quadgram fitted on train dataset:\",perplexity)\n",
        "perplexity_data[\"test_perp_with_LPsmoothing\"].append(perplexity)"
      ],
      "metadata": {
        "id": "UOVBJKfJFde6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Smoothed LMs on train data\n",
        "model = ngram_smoothed(1)\n",
        "model.fit(processed_corpus)\n",
        "perplexity = model.perplexity(processed_corpus)\n",
        "print(\"Average perplexity of test dataset on Laplace smoothed unigram fitted on train dataset:\",perplexity)\n",
        "perplexity_data[\"train_perp_with_LPsmoothing\"].append(perplexity)\n",
        "\n",
        "print(\"---------------------------------------------------------\")\n",
        "\n",
        "model = ngram_smoothed(2)\n",
        "model.fit(processed_corpus)\n",
        "perplexity = model.perplexity(processed_corpus)\n",
        "print(\"Average perplexity of test dataset on Laplace smoothed bigram fitted on train dataset:\",perplexity)\n",
        "perplexity_data[\"train_perp_with_LPsmoothing\"].append(perplexity)\n",
        "\n",
        "print(\"---------------------------------------------------------\")\n",
        "\n",
        "model = ngram_smoothed(3)\n",
        "model.fit(processed_corpus)\n",
        "perplexity = model.perplexity(processed_corpus)\n",
        "print(\"Average perplexity of test dataset on Laplace smoothed trigram fitted on train dataset:\",perplexity)\n",
        "perplexity_data[\"train_perp_with_LPsmoothing\"].append(perplexity)\n",
        "\n",
        "print(\"---------------------------------------------------------\")\n",
        "\n",
        "model = ngram_smoothed(4)\n",
        "model.fit(processed_corpus)\n",
        "perplexity = model.perplexity(processed_corpus)\n",
        "print(\"Average perplexity of test dataset on Laplace smoothed quadgram fitted on train dataset:\",perplexity)\n",
        "perplexity_data[\"train_perp_with_LPsmoothing\"].append(perplexity)\n",
        "\n",
        "print(\"---------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "5PahpTr8PKB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plotting the trend of average perplexity scores on the train dataset on Unsmoothed Language models"
      ],
      "metadata": {
        "id": "0smcwWeCKKfh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "perp_df = pd.DataFrame(perplexity_data)\n",
        "perp_df.index = [\"unigram\", \"bigram\", \"trigram\", \"quadgram\"]\n",
        "print(perp_df)"
      ],
      "metadata": {
        "id": "N-4uFw1mKVr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(8, 8))\n",
        "\n",
        "\n",
        "ax1.plot(perp_df.index, perplexity_data[\"train_perp_without_LPsmoothing\"], label=\"Train Perplexity without LP Smoothing\")\n",
        "ax1.set_xlabel(\"N-Gram LMs\")\n",
        "ax1.set_ylabel(\"Average Perplexity\")\n",
        "ax1.legend()\n",
        "\n",
        "# Plot train_perp_without_LPsmoothing and train_perp_with_LPsmoothing on another plot\n",
        "ax2.plot(perp_df.index, perplexity_data[\"train_perp_with_LPsmoothing\"], label=\"Train Perplexity with LP Smoothing\")\n",
        "ax2.set_xlabel(\"N-Gram LMs\")\n",
        "ax2.set_ylabel(\"Average Perplexity\")\n",
        "ax2.legend()\n",
        "\n",
        "ax3.plot(perp_df.index, perplexity_data[\"test_perp_with_LPsmoothing\"], label=\"Test Perplexity with LP Smoothing\")\n",
        "ax3.set_xlabel(\"N-Gram LMs\")\n",
        "ax3.set_ylabel(\"Average Perplexity\")\n",
        "ax3.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "# Show the plots\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "q4zOCbXjSiXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PpTjPa-tTf_1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}